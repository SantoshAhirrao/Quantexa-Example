package com.quantexa.example.etl.projects.fiu.customer

import java.io.FileInputStream
import com.quantexa.engSpark.model.EngModel.NetworkOutput
import com.quantexa.engSpark.utils.ExplorerFormatter._
import com.quantexa.investigation.api.GraphCustodianProtocol.InitialiseGraphResponse
import com.quantexa.quanfiguration.ResolverConfigReader
import com.quantexa.quanfiguration.utils.ResolverConfigUtils._
import com.quantexa.scriptrunner.{NoConfigSparkScript, QuantexaSparkScript}
import com.quantexa.scriptrunner.util.metrics.ETLMetricsRepository
import com.quantexa.example.etl.projects.fiu.rest.InvestigationRestClient._
import org.apache.log4j.Logger
import org.apache.spark.sql.{Encoder, Encoders, SparkSession}
import io.circe.syntax._
import io.circe.generic.auto._

/***
  * QuantexaSparkScript used to create investigations for networks generated by Batch Resolver
  */
object CreateInvestigations extends NoConfigSparkScript {
  override def name: String = "FIU Investigation Loading"

  val namePrefix = "FIU_Investigation_"
  val lazyInit = Some(true)
  val natures: None.type = None

  val resolutionTemplate = "default"

  val fileDependencies: Map[String, String] = Map.empty
  val scriptDependencies = Set.empty[QuantexaSparkScript]

  def run(spark: SparkSession, logger: Logger, args: Seq[String], etlMetricsRepository: ETLMetricsRepository): Unit = {
    import spark.implicits._
    implicit val entityGraphEncoder: Encoder[Either[Throwable, InitialiseGraphResponse]] = Encoders.kryo[Either[Throwable, InitialiseGraphResponse]]

    if (args.length != 6) {
      throw new Exception("Please supply <resolve-config.json> <engRootDirectory> <loginAPIURL> <explorerAPIURL> <batchAPIUsername> <batchAPIPassword>")
    }

    val (resolveConfigPath,engRootDirectory,loginAPIURL,explorerAPIURL,username,password) = (args.head,args(1),args(2),args(3),args(4),args(5))

    //TODO: This is a bug workaround as in IP-621. The entities should be taken from the resolver config again when it is fixed
//    val resolverConfig = ResolverConfigReader.read(new FileInputStream(resolveConfigPath))
//      .getOrElse(throw new IllegalArgumentException("Invalid Resolver Config"))

    //Load the ENG Networks
    val networksDS = spark
      .read
      .parquet(s"$engRootDirectory/NetworkBuild/network_output.parquet")
      .distinct
      .as[NetworkOutput]

    //Project specific logic here to subset the Networks to the networks you wish to create investigations for
    val networksSubsetDS =
      networksDS
        .limit(20)

    // Gather a list of entities from the supplied configuration via the getEntityConfiguration utility function
//    val definedEntities = getEntityConfiguration(resolverConfig,resolutionTemplate).keys.toSeq.distinct
    val definedEntities = Seq("address", "business", "individual", "telephone", "account")

    // Convert the NetworkOutput -> (ResolverRequest, NetworkID)
    val resolverRequestNetworkIdDS = formatNetworks(networksSubsetDS, resolutionTemplates(definedEntities,resolutionTemplate))

    // Convert all the investigations to Json for loading
    val investigationRequestDS = resolverRequestNetworkIdDS.map{
      case (resolverRequest,networkID) =>
        InvestigationRequest(
          resolverRequest
          , lazyInit
          , Some(namePrefix+networkID)
          , natures
        ).asJson
    }

    //Count the number of investigations which failed to load if any
    val rowCount = investigationRequestDS.count
    logger.info(s"Posting $rowCount rows to explorer database")

    //Loads each investigation in the dataset into the UI
    val result = investigationRequestDS.map(investigation => loadInvestigation(investigation, loginAPIURL, explorerAPIURL, username, password).attempt.unsafeRunSync())

    //Outputs the error information if there are any
    val failures = result.filter(_.isLeft).map(_.left.get.getMessage)
    val numberOfFailures = failures.count

    if(numberOfFailures == 0){
      logger.info(s"All requests succeeded!")
    }else{
      logger.info(s"${rowCount - numberOfFailures} requests succeeded!")
      logger.info(s"$numberOfFailures requests didn't succeed!")
      failures
        .groupBy(failures.columns.head)
        .count
        .foreach{row => logger.info(s"There were ${row(1)} exceptions with the following message:\r\n ${row(0)}")}
    }
  }
}